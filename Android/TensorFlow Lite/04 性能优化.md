# 性能优化

 移动和嵌入式设备的计算资源有限，保持应用程序资源的有效性非常重要。 我们编制了一份最佳实践和策略列表，您可以在使用Tensorflow Lite时优化模型和应用程序。

## 为任务选择最佳模型

 根据任务，您需要在模型复杂性和大小之间进行权衡。 如果您的任务需要高精度，那么您可能需要一个庞大而复杂的模型。 某些任务可能使用较不精确的模型，对于这些任务，最好使用较小但不太精确的模型。 较小的型号不仅使用较少的磁盘空间和内存，而且通常更快，更节能。

 针对移动设备优化的模型的一个示例是[MobileNets](https://arxiv.org/abs/1704.04861)，其针对移动视觉应用进行了优化。 Tensorflow Lite[模型页面](https://www.tensorflow.org/lite/models)列出了其他几种专为移动和嵌入式设备优化的型号。

 您可以使用转移学习在您自己的数据集上重新训练列出的模型。查看我们的[图像分类](https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#0)和[对象检测](https://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193)的转移学习教程。

## 分析模型

 选择适合您任务的候选模型后，最好对模型进行分析和基准测试。 Tensorflow Lite基准测试工具具有内置的分析器，可显示每个运算的分析统计信息。 这有助于理解性能瓶颈以及哪些运算主导计算时间。

## 在图表中分析和优化运算

 如果特定运算经常出现在模型中并且基于分析，您会发现运算花费了大量时间，您可以考虑优化运算。 这种情况应该很少见，因为Tensorflow Lite已针对大多数操作优化了版本。 但是，如果您知道执行运算符的约束，则可以编写更快版本的自定义op。 查看我们的[自定义运算文档](https://www.tensorflow.org/lite/custom_operators)。

## 量化您的模型

 如果您的模型使用浮点权重或活跃，则可以通过使用量化和其他模型优化将模型的大小减小到~4倍。 查看我们的[模型优化工具包](https://www.tensorflow.org/performance/model_optimization)，了解有关优化模型的详细信息。

## 调整线程数

 Tensorflow Lite支持许多运算的多线程内核。 您可以增加线程数并加快运算符的执行速度。 但是，增加线程数会使您的模型使用更多资源和功耗。 对于某些应用，延迟可能比能效更重要。 您可以通过设置解释器线程数来增加线程数。 然而，多线程执行的代价是增加的性能可变性，这取决于同时执行的其他内容。 移动应用尤其如此。 例如，隔离测试可能比单线程加速2倍，但如果同时执行另一个应用程序可能会导致性能低于单线程。

## 消除冗余副本

 如果您的应用程序不小心，在将输入馈送到模型并从模型读取输出时可能会有冗余副本。 确保消除冗余副本。 如果您使用的是更高级别的API（如Java API），请务必仔细检查文档以获取性能警告。 例如，如果将ByteBuffers用作[输入](https://github.com/tensorflow/tensorflow/blob/6305a6d83552ba6a472cd72398b60d9241467f1f/tensorflow/contrib/lite/java/src/main/java/org/tensorflow/lite/Interpreter.java#L151)，则Java API会快得多。

## 使用特定于平台的工具分析您的应用

 Android Profiler和Instruments等平台特定工具提供了大量可用于调试应用程序的分析信息。 有时，性能错误可能不在模型中，而是在与模型交互的应用程序代码的部分中。 确保熟悉平台特定的分析工具和平台的最佳实践。

## 评估您的模型是否受益于使用设备上可用的硬件加速器

 Tensorflow Lite正在努力增加对GPU等加速器的支持，并通过Android上的[Neural Networks API](https://developer.android.com/ndk/guides/neuralnetworks/)提供加速。 您可以利用这些硬件加速器后端来提高模型的速度和效率。 要启用Neural Networks API，请在解释器实例上调用[UseNNAPI](https://github.com/tensorflow/tensorflow/blob/6305a6d83552ba6a472cd72398b60d9241467f1f/tensorflow/contrib/lite/interpreter.h#L334)。